{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glossary.Glossary import Glossary\n",
    "import uuid\n",
    "from glossary.glossaries.pretty_glossary import pretty_core\n",
    "glossary = Glossary(pretty_core)\n",
    "import pymongo\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import collections\n",
    "import urllib\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import textacy\n",
    "from urllib.request import Request, urlopen\n",
    "import textacy.keyterms as tkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_connection():\n",
    "    myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "    mydb = myclient[\"parent_child1\"]\n",
    "    mycol = mydb[\"tags\"]\n",
    "    return mycol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 'index'\n",
    "#input : list of dict, key for index\n",
    "#output : dict with value = dict from input, key = key for index\n",
    "def build_dict(seq, key):\n",
    "    return dict((d[key], dict(d, index=index)) for (index, d) in enumerate(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_str_name_id(sort_glos):\n",
    "    name_id=[]\n",
    "    for i in sort_glos:\n",
    "        doc = {'_id' : str(uuid.uuid4()), 'name' : i}\n",
    "        name_id.append(doc)\n",
    "    return name_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_doc_for_mongo(sort_glos,final):\n",
    "    for i in sort_glos:\n",
    "        p=glossary.get_parent(i)\n",
    "        c=glossary.get_children(i)\n",
    "        ch = [val for sublist in c for val in sublist]\n",
    "        index = []\n",
    "        id1 = []\n",
    "        id2=[]\n",
    "        for  j in range(len(p)):\n",
    "            index.append(next((index for (index, d) in enumerate(name_id) if d[\"name\"] == p[j]), None))\n",
    "            if any(elem is not None for elem in index) : id1.append(name_id[index[j]].get('_id'))\n",
    "\n",
    "        for  k in range(len(ch)):\n",
    "            child_info = info_by_name.get(ch[k])\n",
    "            id2.append(child_info[\"_id\"])\n",
    "        par = {'parent':id1}\n",
    "        child = {'child':id2}\n",
    "        doc = {'_id' : info_by_name.get(i)['_id'], 'name' : i, 'parent' : id1,'child' : id2}\n",
    "        final.append(doc)\n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert into DB\n",
    "def insert(mycol,final):\n",
    "    x = mycol.insert_many(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching(url='https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8'):\n",
    "    try:\n",
    "        req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urlopen(req).read()\n",
    "    except:\n",
    "        print(\"Smth wrong\")\n",
    "    soup = BeautifulSoup( webpage,'html.parser')\n",
    "    # kill all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    #findng key phrases\n",
    "    text = textacy.preprocess_text(text, lowercase=True, no_punct=True,no_urls=True)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    text = ' '.join(filtered_sentence)\n",
    "    doc = textacy.Doc(text,lang='en_core_web_md')\n",
    "    our_list = list(tkt.sgrank(doc, ngrams=(1, 2, 3), normalize='lower', n_keyterms=0.1, window_width=1500))[:20]\n",
    "    our_list_withot_probs = [i for i,j in our_list]\n",
    "    return our_list,our_list_withot_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category(child, category_for_word):\n",
    "    if str(child.get('name')) not in category :\n",
    "        parent_id = child.get('parent')\n",
    "        #print(parent_id)\n",
    "        for i in parent_id:\n",
    "            #print(i)\n",
    "            parent = mycol.find({'_id':str(i)},{'name':1,'parent':1})\n",
    "            #print(parent)\n",
    "            for j in parent:\n",
    "                #print(j)\n",
    "                #if str(j.get('name')) not in category:\n",
    "                 #   print(j)\n",
    "                get_category(j,category_for_word)\n",
    "                #else : category_for_word.append(j.get(name))\n",
    "    else : \n",
    "        category_for_word.append(child.get('name') )\n",
    "        return category_for_word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_category(category_for_word):\n",
    "    for i in mb_category:\n",
    "        for j in mycol.find({},{'_id':1,'name':1,'parent':1}):\n",
    "            if str(i)==str(j.get('name')):\n",
    "                get_category(j,category_for_word)\n",
    "            else: \n",
    "                s = textacy.similarity.hamming(str(i), str(j.get('name')) )\n",
    "                if s>threshold:\n",
    "\n",
    "                    get_category(j,category_for_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycol = create_connection()\n",
    "sort_glos = sorted(glossary.tags)\n",
    "name_id = create_str_name_id(sort_glos)\n",
    "info_by_name=build_dict(name_id,key = 'name')\n",
    "final =[]\n",
    "create_doc_for_mongo(sort_glos,final)\n",
    "insert(mycol,final)\n",
    "mb_category=matching()[1]\n",
    "category = ['artificial intelligence','deep learning','computer vision','natural language processing']\n",
    "threshold = 0.4\n",
    "category_for_word =[]\n",
    "category_in_text = match_category(category_for_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
